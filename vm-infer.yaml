version: "3.8"

x-minio-env: &minio-env
    MINIO_URL: http://${FIP_OPS}:9000
    MINIO_ACCESS_KEY: minioadmin
    MINIO_SECRET_KEY: minioadmin123

services:
    triton:
        image: nvcr.io/nvidia/tritonserver:23.09-py3
        runtime: nvidia
        shm_size: "16g"  # 增加共享内存大小
        command: >
        tritonserver --model-repository=/models
                    --model-control-mode=poll
                    --http-port=8000
                    --allow-http=true
                    --log-verbose=1  
                    --log-error=true
                    --strict-model-config=false  
                    --model-load-thread-count=4  
                    --exit-on-error=false  
                    --repository-poll-secs=60  

        ports: ["8000:8000", "8002:8002"]
        volumes:
        - /mnt/object/models:/models
        deploy:  # 添加资源限制
        resources:
            limits:
            memory: 32G  
        healthcheck:  
        test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
        interval: 30s
        timeout: 10s
        retries: 3
        start_period: 40s  
        networks: [mlops]
        restart: unless-stopped  
        logging:  
        options:
            max-size: "10m"
            max-file: "3"

    deploy:
        build:
            context: ./deploy
            dockerfile: Dockerfile.deploy
        environment:
            <<: *minio-env
        volumes:
            - /mnt/object/models:/app/models
            - /mnt/object/tokenizer:/app/tokenizer
            - ./deploy_data:/app/deploy_data
        ports: ["8080:8000"]
        depends_on: [triton]

        # cpus: "8"
        # cpu_shares: 512
        restart: always
        networks: [mlops]

    monitor:
        build:
            context: ./monitor
            dockerfile: Dockerfile.monitor
        environment:
            <<: *minio-env
            ETL_URL: http://${FIP_TRAIN}:8010
        volumes:
            - ./deploy_data:/app/deploy_data
            - ./monitor_data:/app/monitor_data
        ports: ["8030:8000"]
        depends_on: [deploy]
        # cpus: "8"
        # cpu_shares: 512
        restart: always
        networks: [mlops]

networks:
    mlops:
        driver: bridge
