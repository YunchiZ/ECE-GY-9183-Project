{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import wandb\n",
    "import datasets\n",
    "from ray import tune\n",
    "import ray\n",
    "import os\n",
    "from ray.air import session\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since abisee/cnn_dailymail couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration '3.0.0' at dataset/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d (last modified on Tue Apr 15 09:04:36 2025).\n"
     ]
    }
   ],
   "source": [
    "save_path = './dataset'\n",
    "dataset = load_dataset('abisee/cnn_dailymail', '3.0.0', cache_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name, cache_dir='./model')\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, cache_dir='./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 冻结编码器部分的所有层\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 仅训练解码器部分的层\n",
    "for param in model.model.decoder.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"article\"]\n",
    "    targets = examples[\"highlights\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"article\"]\n",
    "    targets = examples[\"highlights\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # 显式保留原始文本列\n",
    "    model_inputs[\"article\"] = examples[\"article\"]\n",
    "    model_inputs[\"highlights\"] = examples[\"highlights\"]\n",
    "    return model_inputs\n",
    "\n",
    "# 应用预处理函数时，仅移除不需要的列（如果有）\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[]  # 移除原始数据集中其他不需要的列（如果有）\n",
    "    # 如果不需要移除任何列，可以设置为 remove_columns=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"highlights\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets.save_to_disk(\"./dataset/tokenized_datasets_2\")\n",
    "# tokenized_datasets = datasets.load_from_disk(\"./dataset/tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "def compute_metrics(pred):\n",
    "    try:\n",
    "        rouge = evaluate.load('rouge')\n",
    "        \n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "        \n",
    "        # 如果predictions是一个包含logits的数组，取argmax\n",
    "        if len(pred_ids.shape) == 3:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "        \n",
    "        # 处理-100（特殊填充值）\n",
    "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "        \n",
    "        # 解码为文本\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # 计算ROUGE分数\n",
    "        result = rouge.compute(predictions=pred_str, references=label_str, use_stemmer=True)\n",
    "        \n",
    "        # 只返回中间F1分数\n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"].mid.fmeasure,\n",
    "            \"rouge2\": result[\"rouge2\"].mid.fmeasure,\n",
    "            \"rougeL\": result[\"rougeL\"].mid.fmeasure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"计算指标时出错：{e}\")\n",
    "        # 返回默认值以避免训练中断\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     run_name = \"Epoch_1_test\",\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_total_limit=3,\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_steps=100,\n",
    "#     fp16=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_size = 1000  # You can adjust this number to use fewer examples\n",
    "eval_size = 200  # Same for evaluation dataset\n",
    "\n",
    "train_subset = tokenized_datasets[\"train\"].select(range(train_size))\n",
    "eval_subset = tokenized_datasets[\"validation\"].select(range(eval_size))\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,  # Your pre-trained model\n",
    "#     args=training_args,  # Pass the training arguments\n",
    "#     # train_dataset=tokenized_datasets[\"train\"], \n",
    "#     # eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     train_dataset=train_subset,\n",
    "#     eval_dataset=eval_subset,\n",
    "#     tokenizer=tokenizer,  # Pass the tokenizer\n",
    "#     # compute_metrics=compute_metrics,\n",
    "# )\n",
    "# wandb.init(project=\"Mlops-summary\", entity=\"yunchiz-new-york-university\")\n",
    "# trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(config, model, train_dataset, eval_dataset):\n",
    "    try:\n",
    "        trial_dir = session.get_trial_dir()  # 例如：~/ray_results/test/trial_xxx/\n",
    "        output_dir = os.path.join(trial_dir, \"results\")\n",
    "    except Exception as e:\n",
    "        print(f\"路径错误: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        run_name = \"ray_test_epoch_2\",\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=2,  \n",
    "        \n",
    "        # per_device_train_batch_size=config[\"batch_size\"],  # Hyperparameter from Ray Tune\n",
    "        # per_device_eval_batch_size=config[\"batch_size\"],   # Hyperparameter from Ray Tune\n",
    "        # gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],               # Hyperparameter from Ray Tune\n",
    "\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        learning_rate=config[\"learning_rate\"],              # Hyperparameter from Ray Tune\n",
    "        \n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(trial_dir, \"logs\"),  \n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "\n",
    "        save_total_limit=3,\n",
    "        metric_for_best_model=\"rougeL\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=train_subset, \n",
    "        eval_dataset=eval_subset, \n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    try:\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"训练失败: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "    # Evaluate the model\n",
    "        eval_results = trainer.evaluate()\n",
    "    except Exception as e:\n",
    "        print(f\"评估失败: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "    # Return the evaluation results to Ray Tune\n",
    "        tune.report(metrics=eval_results)\n",
    "        trainer.save_model(output_dir)\n",
    "        tune.report(\n",
    "            metrics=eval_results,\n",
    "            checkpoint=tune.Checkpoint.from_directory(output_dir)  # 将模型目录作为检查点\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"报告错误: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    # \"learning_rate\": tune.grid_search([1e-5, 2e-5, 5e-5]),\n",
    "    # \"batch_size\": tune.choice([8, 16]),\n",
    "    # \"warmup_steps\": tune.choice([500, 1000, 2000]),\n",
    "    \"learning_rate\": tune.grid_search([1e-5]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myunchiz\u001b[0m (\u001b[33myunchiz-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/wandb/run-20250415_104734-qcpiyjpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl' target=\"_blank\">scarlet-smoke-24</a></strong> to <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f40ee2353a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Mlops-summary\", entity=\"yunchiz-new-york-university\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-15 10:50:34</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:45.58        </td></tr>\n",
       "<tr><td>Memory:      </td><td>34.1/629.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/10 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  eval_loss</th><th style=\"text-align: right;\">  eval_rouge1</th><th style=\"text-align: right;\">  eval_rouge2</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_04641_00000</td><td>TERMINATED</td><td>172.17.0.2:15050</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         141.888</td><td style=\"text-align: right;\">    1.32029</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">            0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Currently logged in as: yunchiz (yunchiz-new-york-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Tracking run with wandb version 0.19.9\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-04-15_10-47-35_553542_13907/artifacts/2025-04-15_10-47-48/ray_test_epoch_2/working_dirs/train_fn_04641_00000_0_learning_rate=0.0000_2025-04-15_10-47-48/wandb/run-20250415_104805-q97xhpey\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Syncing run ray_test_epoch_2\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/yunchiz-new-york-university/huggingface\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: 🚀 View run at https://wandb.ai/yunchiz-new-york-university/huggingface/runs/q97xhpey\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]\n",
      "  2%|▏         | 1/62 [00:01<01:49,  1.79s/it]\n",
      "  3%|▎         | 2/62 [00:02<01:09,  1.16s/it]\n",
      "  5%|▍         | 3/62 [00:03<01:00,  1.03s/it]\n",
      "  6%|▋         | 4/62 [00:04<00:50,  1.15it/s]\n",
      "  8%|▊         | 5/62 [00:04<00:44,  1.28it/s]\n",
      " 10%|▉         | 6/62 [00:05<00:40,  1.38it/s]\n",
      " 11%|█▏        | 7/62 [00:05<00:37,  1.45it/s]\n",
      " 13%|█▎        | 8/62 [00:06<00:35,  1.51it/s]\n",
      " 15%|█▍        | 9/62 [00:07<00:34,  1.54it/s]\n",
      " 16%|█▌        | 10/62 [00:07<00:33,  1.57it/s]\n",
      " 18%|█▊        | 11/62 [00:08<00:32,  1.57it/s]\n",
      " 19%|█▉        | 12/62 [00:08<00:31,  1.58it/s]\n",
      " 21%|██        | 13/62 [00:09<00:30,  1.59it/s]\n",
      " 23%|██▎       | 14/62 [00:10<00:30,  1.59it/s]\n",
      " 24%|██▍       | 15/62 [00:10<00:29,  1.60it/s]\n",
      " 26%|██▌       | 16/62 [00:11<00:28,  1.60it/s]\n",
      " 27%|██▋       | 17/62 [00:12<00:27,  1.61it/s]\n",
      " 29%|██▉       | 18/62 [00:12<00:27,  1.61it/s]\n",
      " 31%|███       | 19/62 [00:13<00:26,  1.61it/s]\n",
      " 32%|███▏      | 20/62 [00:13<00:26,  1.61it/s]\n",
      " 34%|███▍      | 21/62 [00:14<00:25,  1.61it/s]\n",
      " 35%|███▌      | 22/62 [00:15<00:24,  1.61it/s]\n",
      " 37%|███▋      | 23/62 [00:15<00:24,  1.61it/s]\n",
      " 39%|███▊      | 24/62 [00:16<00:23,  1.61it/s]\n",
      " 40%|████      | 25/62 [00:17<00:22,  1.61it/s]\n",
      " 42%|████▏     | 26/62 [00:17<00:22,  1.61it/s]\n",
      " 44%|████▎     | 27/62 [00:18<00:21,  1.61it/s]\n",
      " 45%|████▌     | 28/62 [00:18<00:21,  1.61it/s]\n",
      " 47%|████▋     | 29/62 [00:19<00:20,  1.61it/s]\n",
      " 48%|████▊     | 30/62 [00:20<00:19,  1.60it/s]\n",
      " 50%|█████     | 31/62 [00:20<00:19,  1.59it/s]\n",
      " 52%|█████▏    | 32/62 [00:20<00:14,  2.05it/s]\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 12%|█▏        | 3/25 [00:00<00:00, 22.88it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 24%|██▍       | 6/25 [00:00<00:01, 17.60it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 32%|███▏      | 8/25 [00:00<00:01,  9.72it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 40%|████      | 10/25 [00:00<00:01, 10.96it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 48%|████▊     | 12/25 [00:00<00:01, 11.92it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 56%|█████▌    | 14/25 [00:01<00:00, 12.65it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 64%|██████▍   | 16/25 [00:01<00:00, 13.11it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 72%|███████▏  | 18/25 [00:01<00:00,  8.89it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 80%|████████  | 20/25 [00:01<00:00,  7.92it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 88%|████████▊ | 22/25 [00:02<00:00,  8.62it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 96%|█████████▌| 24/25 [00:02<00:00,  5.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m 计算指标时出错：'tuple' object has no attribute 'shape'\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m {'eval_loss': 1.9222586154937744, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_runtime': 11.9726, 'eval_samples_per_second': 16.705, 'eval_steps_per_second': 2.088, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 52%|█████▏    | 32/62 [00:32<00:14,  2.05it/s]\n",
      "100%|██████████| 25/25 [00:11<00:00,  5.76it/s]\u001b[A\n",
      "                                               \u001b[A/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m   warnings.warn(\n",
      " 53%|█████▎    | 33/62 [00:40<02:59,  6.18s/it]\n",
      " 55%|█████▍    | 34/62 [00:41<02:06,  4.51s/it]\n",
      " 56%|█████▋    | 35/62 [00:41<01:30,  3.35s/it]\n",
      " 58%|█████▊    | 36/62 [00:42<01:06,  2.57s/it]\n",
      " 60%|█████▉    | 37/62 [00:43<00:49,  1.99s/it]\n",
      " 61%|██████▏   | 38/62 [00:43<00:38,  1.59s/it]\n",
      " 63%|██████▎   | 39/62 [00:44<00:29,  1.30s/it]\n",
      " 65%|██████▍   | 40/62 [00:44<00:24,  1.11s/it]\n",
      " 66%|██████▌   | 41/62 [00:45<00:20,  1.03it/s]\n",
      " 68%|██████▊   | 42/62 [00:46<00:17,  1.14it/s]\n",
      " 69%|██████▉   | 43/62 [00:46<00:15,  1.24it/s]\n",
      " 71%|███████   | 44/62 [00:47<00:13,  1.32it/s]\n",
      " 73%|███████▎  | 45/62 [00:48<00:12,  1.37it/s]\n",
      " 74%|███████▍  | 46/62 [00:48<00:11,  1.41it/s]\n",
      " 76%|███████▌  | 47/62 [00:49<00:10,  1.45it/s]\n",
      " 77%|███████▋  | 48/62 [00:50<00:09,  1.48it/s]\n",
      " 79%|███████▉  | 49/62 [00:50<00:08,  1.51it/s]\n",
      " 81%|████████  | 50/62 [00:51<00:07,  1.53it/s]\n",
      " 82%|████████▏ | 51/62 [00:52<00:07,  1.54it/s]\n",
      " 84%|████████▍ | 52/62 [00:52<00:06,  1.55it/s]\n",
      " 85%|████████▌ | 53/62 [00:53<00:05,  1.56it/s]\n",
      " 87%|████████▋ | 54/62 [00:54<00:05,  1.56it/s]\n",
      " 89%|████████▊ | 55/62 [00:54<00:04,  1.56it/s]\n",
      " 90%|█████████ | 56/62 [00:55<00:03,  1.56it/s]\n",
      " 92%|█████████▏| 57/62 [00:55<00:03,  1.56it/s]\n",
      " 94%|█████████▎| 58/62 [00:56<00:02,  1.55it/s]\n",
      " 95%|█████████▌| 59/62 [00:57<00:01,  1.55it/s]\n",
      " 97%|█████████▋| 60/62 [00:57<00:01,  1.55it/s]\n",
      " 98%|█████████▊| 61/62 [00:58<00:00,  1.55it/s]\n",
      "100%|██████████| 62/62 [00:59<00:00,  1.55it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 12%|█▏        | 3/25 [00:00<00:00, 22.54it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 24%|██▍       | 6/25 [00:00<00:01, 17.59it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 32%|███▏      | 8/25 [00:00<00:01, 16.41it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 40%|████      | 10/25 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 48%|████▊     | 12/25 [00:00<00:00, 15.24it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 56%|█████▌    | 14/25 [00:00<00:00, 14.97it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 64%|██████▍   | 16/25 [00:01<00:00, 14.65it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 72%|███████▏  | 18/25 [00:01<00:00, 14.39it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 80%|████████  | 20/25 [00:01<00:00, 14.21it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 88%|████████▊ | 22/25 [00:01<00:00, 14.09it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 96%|█████████▌| 24/25 [00:01<00:00, 14.00it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m 计算指标时出错：'tuple' object has no attribute 'shape'\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m {'eval_loss': 1.3202884197235107, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_runtime': 10.3793, 'eval_samples_per_second': 19.269, 'eval_steps_per_second': 2.409, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 62/62 [01:09<00:00,  1.55it/s]\n",
      "100%|██████████| 25/25 [00:10<00:00, 14.00it/s]\u001b[A\n",
      "                                               \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m {'train_runtime': 82.2217, 'train_samples_per_second': 24.324, 'train_steps_per_second': 0.754, 'train_loss': 3.1087245325888357, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:20<00:00,  1.30s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      " 12%|█▏        | 3/25 [00:00<00:01, 17.42it/s]\n",
      " 20%|██        | 5/25 [00:00<00:01, 15.56it/s]\n",
      " 28%|██▊       | 7/25 [00:00<00:01, 15.34it/s]\n",
      " 36%|███▌      | 9/25 [00:00<00:01, 15.04it/s]\n",
      " 44%|████▍     | 11/25 [00:00<00:00, 14.78it/s]\n",
      " 52%|█████▏    | 13/25 [00:00<00:00, 14.69it/s]\n",
      " 60%|██████    | 15/25 [00:01<00:00, 14.62it/s]\n",
      " 68%|██████▊   | 17/25 [00:01<00:00, 14.57it/s]\n",
      " 76%|███████▌  | 19/25 [00:01<00:00, 14.46it/s]\n",
      " 84%|████████▍ | 21/25 [00:01<00:00, 14.32it/s]\n",
      " 92%|█████████▏| 23/25 [00:01<00:00, 14.15it/s]\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m 计算指标时出错：'tuple' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:09<00:00,  2.57it/s]\n",
      "2025-04-15 10:50:10,267\tWARNING experiment_state.py:233 -- Saving the experiment state (which holds a global view of trial statuses and is used to restore the experiment) has already taken 30.24 seconds, which may cause consistency issues upon restoration if your driver script ungracefully exits.\n",
      "This could be due to a large number of trials, large logfiles from lots of reported metrics, or throttling from the remote storage if uploading too frequently.\n",
      "You may want to consider switching the `RunConfig(storage_filesystem)` to a more performant storage backend such as s3fs for a S3 storage path.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_SLOW_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a higher value than the current threshold (30.0).\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/autodl-tmp/ray_results/ray_test_epoch_2/train_fn_04641_00000_0_learning_rate=0.0000_2025-04-15_10-47-48/checkpoint_000000)\n",
      "2025-04-15 10:50:34,341\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/autodl-tmp/ray_results/ray_test_epoch_2' in 16.0637s.\n",
      "2025-04-15 10:50:34,351\tINFO tune.py:1041 -- Total run time: 165.76 seconds (149.51 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "storage_path = f\"file://{current_dir}/ray_results\"\n",
    "\n",
    "train_fn_with_params = tune.with_parameters(train_fn, model=model, train_dataset=train_subset, eval_dataset=eval_subset)\n",
    "ray.init(ignore_reinit_error=True)  # Initialize Ray\n",
    "analysis = tune.run(\n",
    "    train_fn_with_params,  # The training function that Ray Tune will use\n",
    "    config=search_space,  # The search space of hyperparameters\n",
    "    # resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
    "    resources_per_trial={\"cpu\": 0, \"gpu\": 1},\n",
    "    num_samples=1,  # Number of trials (hyperparameter combinations)\n",
    "    verbose=1,  # Verbosity level of Ray Tune\n",
    "    storage_path=storage_path,\n",
    "    name=\"ray_test_epoch_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳模型路径：/tmp/checkpoint_tmp_f69b548edfae4ccb947b243d69a34093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_size = 200  # Same for evaluation dataset\n",
    "test_dataset = tokenized_datasets[\"test\"].select(range(test_size))\n",
    "\n",
    "best_trial = analysis.get_best_trial(metric=\"eval_accuracy\", mode=\"max\")\n",
    "\n",
    "# 获取检查点路径（通过 checkpoint 属性）\n",
    "best_checkpoint = best_trial.checkpoint\n",
    "best_checkpoint_dir = best_checkpoint.to_directory()  # 提取检查点目录\n",
    "print(f\"最佳模型路径：{best_checkpoint_dir}\")\n",
    "\n",
    "# 加载模型\n",
    "from transformers import AutoModel\n",
    "best_model = BartForConditionalGeneration.from_pretrained(best_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=best_model,\n",
    "#     args=TrainingArguments(output_dir=\"./tmp\"),  # 临时目录，仅用于预测\n",
    "# )\n",
    "\n",
    "# predictions = trainer.predict(test_dataset)\n",
    "# predictions_logits = predictions.predictions\n",
    "# # predicted_labels = np.argmax(predictions_logits, axis=1)\n",
    "\n",
    "# accuracy = accuracy_score(test_dataset[\"labels\"], predictions_logits)\n",
    "# f1 = f1_score(test_dataset[\"labels\"], predictions_logits, average=\"macro\")\n",
    "\n",
    "# results = rouge.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# print(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": 128,          # 生成摘要的最大长度\n",
    "    \"min_length\": 30,           # 生成摘要的最小长度\n",
    "    \"num_beams\": 4,             # Beam Search 的 beam 数\n",
    "    \"length_penalty\": 2.0,      # 长度惩罚系数（>1鼓励更长，<1鼓励更短）\n",
    "    \"no_repeat_ngram_size\": 3,  # 禁止重复的 n-gram 大小\n",
    "    \"early_stopping\": True,     # 是否提前停止生成\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testset(test_data, model, tokenizer):\n",
    "    \"\"\"\n",
    "    评估测试集并返回 ROUGE 指标\n",
    "    Args:\n",
    "        test_data (Dataset): 预处理后的测试数据集（需包含 \"article\" 和 \"highlights\"）\n",
    "        model (PreTrainedModel): 加载的最佳模型\n",
    "        tokenizer (PreTrainedTokenizer): 对应的 tokenizer\n",
    "    Returns:\n",
    "        dict: ROUGE 指标结果\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 将模型移动到 GPU（如果可用）\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # 存储预测和真实摘要\n",
    "        pred_summaries = []\n",
    "        true_summaries = []\n",
    "        \n",
    "        # 批量生成预测\n",
    "        batch_size = 8  # 根据 GPU 显存调整\n",
    "        for i in range(0, len(test_data), batch_size):\n",
    "            batch = test_data.select(range(i, min(i+batch_size, len(test_data))))\n",
    "            \n",
    "            # 编码输入文本\n",
    "            inputs = tokenizer(\n",
    "                batch[\"article\"],\n",
    "                max_length=1024,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # 生成摘要\n",
    "            with torch.no_grad():\n",
    "                summaries = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    **gen_kwargs\n",
    "                )\n",
    "            \n",
    "            # 解码预测和真实摘要\n",
    "            decoded_preds = tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
    "            decoded_labels = [highlight for highlight in batch[\"highlights\"]]\n",
    "            \n",
    "            pred_summaries.extend(decoded_preds)\n",
    "            true_summaries.extend(decoded_labels)\n",
    "            \n",
    "            print(f\"Processed {i + batch_size}/{len(test_data)} samples\")\n",
    "            \n",
    "        # 计算 ROUGE 指标\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        results = rouge.compute(\n",
    "            predictions=pred_summaries,\n",
    "            references=true_summaries,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # 格式化结果（保留4位小数）\n",
    "        return {\n",
    "            \"rouge1\": round(results[\"rouge1\"], 4),\n",
    "            \"rouge2\": round(results[\"rouge2\"], 4),\n",
    "            \"rougeL\": round(results[\"rougeL\"], 4),\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"评估过程中发生错误: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8/200 samples\n",
      "Processed 16/200 samples\n",
      "Processed 24/200 samples\n",
      "Processed 32/200 samples\n",
      "Processed 40/200 samples\n",
      "Processed 48/200 samples\n",
      "Processed 56/200 samples\n",
      "Processed 64/200 samples\n",
      "Processed 72/200 samples\n",
      "Processed 80/200 samples\n",
      "Processed 88/200 samples\n",
      "Processed 96/200 samples\n",
      "Processed 104/200 samples\n",
      "Processed 112/200 samples\n",
      "Processed 120/200 samples\n",
      "Processed 128/200 samples\n",
      "Processed 136/200 samples\n",
      "Processed 144/200 samples\n",
      "Processed 152/200 samples\n",
      "Processed 160/200 samples\n",
      "Processed 168/200 samples\n",
      "Processed 176/200 samples\n",
      "Processed 184/200 samples\n",
      "Processed 192/200 samples\n",
      "Processed 200/200 samples\n",
      "\n",
      "测试集评估结果:\n",
      "ROUGE-1: 0.3567\n",
      "ROUGE-2: 0.1528\n",
      "ROUGE-L: 0.263\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 加载测试数据集（假设已预处理）\n",
    "    # test_size = 200\n",
    "    # test_dataset = tokenized_datasets[\"test\"].select(range(test_size))\n",
    "    \n",
    "    # # 加载最佳模型检查点\n",
    "    # best_trial = analysis.get_best_trial(metric=\"metrics\", mode=\"max\")  # 请根据实际指标名称修改\n",
    "    # best_checkpoint_dir = best_trial.checkpoint.to_directory()\n",
    "    \n",
    "    # # 加载模型和 tokenizer\n",
    "    # tokenizer = BartTokenizer.from_pretrained(best_checkpoint_dir)\n",
    "    # model = BartForConditionalGeneration.from_pretrained(best_checkpoint_dir)\n",
    "    \n",
    "    # 执行评估\n",
    "    results = evaluate_testset(test_dataset, best_model, tokenizer)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"\\n测试集评估结果:\")\n",
    "    print(f\"ROUGE-1: {results['rouge1']}\")\n",
    "    print(f\"ROUGE-2: {results['rouge2']}\")\n",
    "    print(f\"ROUGE-L: {results['rougeL']}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"数据加载错误: 请检查数据集是否包含 'article' 和 'highlights' 字段 - {str(e)}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"模型加载错误: 检查点路径 {best_checkpoint_dir} 不存在 - {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"未知错误: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-smoke-24</strong> at: <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl</a><br> View project at: <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_104734-qcpiyjpl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def summarize(text):\n",
    "#     inputs = tokenizer([text], max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "#     summary_ids = best_model.generate(inputs[\"input_ids\"], max_length=128, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# print(summarize(\"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "# A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "# Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "# In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "# Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "# 2010 marriage license application, according to court documents.\n",
    "# Prosecutors said the marriages were part of an immigration scam.\n",
    "# On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "# After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "# Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "# All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "# Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "# Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "# The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "# Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "# Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "# If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. If convicted, she faces up to four years in prison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
