{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import wandb\n",
    "import datasets\n",
    "from ray import tune\n",
    "import ray\n",
    "import os\n",
    "from ray.air import session\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since abisee/cnn_dailymail couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration '3.0.0' at dataset/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d (last modified on Tue Apr 15 09:04:36 2025).\n"
     ]
    }
   ],
   "source": [
    "save_path = './dataset'\n",
    "dataset = load_dataset('abisee/cnn_dailymail', '3.0.0', cache_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name, cache_dir='./model')\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, cache_dir='./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å†»ç»“ç¼–ç å™¨éƒ¨åˆ†çš„æ‰€æœ‰å±‚\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ä»…è®­ç»ƒè§£ç å™¨éƒ¨åˆ†çš„å±‚\n",
    "for param in model.model.decoder.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"article\"]\n",
    "    targets = examples[\"highlights\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"article\"]\n",
    "    targets = examples[\"highlights\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # æ˜¾å¼ä¿ç•™åŸå§‹æ–‡æœ¬åˆ—\n",
    "    model_inputs[\"article\"] = examples[\"article\"]\n",
    "    model_inputs[\"highlights\"] = examples[\"highlights\"]\n",
    "    return model_inputs\n",
    "\n",
    "# åº”ç”¨é¢„å¤„ç†å‡½æ•°æ—¶ï¼Œä»…ç§»é™¤ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[]  # ç§»é™¤åŸå§‹æ•°æ®é›†ä¸­å…¶ä»–ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    # å¦‚æœä¸éœ€è¦ç§»é™¤ä»»ä½•åˆ—ï¼Œå¯ä»¥è®¾ç½®ä¸º remove_columns=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"highlights\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets.save_to_disk(\"./dataset/tokenized_datasets_2\")\n",
    "# tokenized_datasets = datasets.load_from_disk(\"./dataset/tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "def compute_metrics(pred):\n",
    "    try:\n",
    "        rouge = evaluate.load('rouge')\n",
    "        \n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "        \n",
    "        # å¦‚æœpredictionsæ˜¯ä¸€ä¸ªåŒ…å«logitsçš„æ•°ç»„ï¼Œå–argmax\n",
    "        if len(pred_ids.shape) == 3:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "        \n",
    "        # å¤„ç†-100ï¼ˆç‰¹æ®Šå¡«å……å€¼ï¼‰\n",
    "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "        \n",
    "        # è§£ç ä¸ºæ–‡æœ¬\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # è®¡ç®—ROUGEåˆ†æ•°\n",
    "        result = rouge.compute(predictions=pred_str, references=label_str, use_stemmer=True)\n",
    "        \n",
    "        # åªè¿”å›ä¸­é—´F1åˆ†æ•°\n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"].mid.fmeasure,\n",
    "            \"rouge2\": result[\"rouge2\"].mid.fmeasure,\n",
    "            \"rougeL\": result[\"rougeL\"].mid.fmeasure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™ï¼š{e}\")\n",
    "        # è¿”å›é»˜è®¤å€¼ä»¥é¿å…è®­ç»ƒä¸­æ–­\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     run_name = \"Epoch_1_test\",\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_total_limit=3,\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_steps=100,\n",
    "#     fp16=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_size = 1000  # You can adjust this number to use fewer examples\n",
    "eval_size = 200  # Same for evaluation dataset\n",
    "\n",
    "train_subset = tokenized_datasets[\"train\"].select(range(train_size))\n",
    "eval_subset = tokenized_datasets[\"validation\"].select(range(eval_size))\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,  # Your pre-trained model\n",
    "#     args=training_args,  # Pass the training arguments\n",
    "#     # train_dataset=tokenized_datasets[\"train\"], \n",
    "#     # eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     train_dataset=train_subset,\n",
    "#     eval_dataset=eval_subset,\n",
    "#     tokenizer=tokenizer,  # Pass the tokenizer\n",
    "#     # compute_metrics=compute_metrics,\n",
    "# )\n",
    "# wandb.init(project=\"Mlops-summary\", entity=\"yunchiz-new-york-university\")\n",
    "# trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(config, model, train_dataset, eval_dataset):\n",
    "    try:\n",
    "        trial_dir = session.get_trial_dir()  # ä¾‹å¦‚ï¼š~/ray_results/test/trial_xxx/\n",
    "        output_dir = os.path.join(trial_dir, \"results\")\n",
    "    except Exception as e:\n",
    "        print(f\"è·¯å¾„é”™è¯¯: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        run_name = \"ray_test_epoch_2\",\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=2,  \n",
    "        \n",
    "        # per_device_train_batch_size=config[\"batch_size\"],  # Hyperparameter from Ray Tune\n",
    "        # per_device_eval_batch_size=config[\"batch_size\"],   # Hyperparameter from Ray Tune\n",
    "        # gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],               # Hyperparameter from Ray Tune\n",
    "\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        learning_rate=config[\"learning_rate\"],              # Hyperparameter from Ray Tune\n",
    "        \n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(trial_dir, \"logs\"),  \n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "\n",
    "        save_total_limit=3,\n",
    "        metric_for_best_model=\"rougeL\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=train_subset, \n",
    "        eval_dataset=eval_subset, \n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    try:\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(f\"è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "    # Evaluate the model\n",
    "        eval_results = trainer.evaluate()\n",
    "    except Exception as e:\n",
    "        print(f\"è¯„ä¼°å¤±è´¥: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "    # Return the evaluation results to Ray Tune\n",
    "        tune.report(metrics=eval_results)\n",
    "        trainer.save_model(output_dir)\n",
    "        tune.report(\n",
    "            metrics=eval_results,\n",
    "            checkpoint=tune.Checkpoint.from_directory(output_dir)  # å°†æ¨¡å‹ç›®å½•ä½œä¸ºæ£€æŸ¥ç‚¹\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"æŠ¥å‘Šé”™è¯¯: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    # \"learning_rate\": tune.grid_search([1e-5, 2e-5, 5e-5]),\n",
    "    # \"batch_size\": tune.choice([8, 16]),\n",
    "    # \"warmup_steps\": tune.choice([500, 1000, 2000]),\n",
    "    \"learning_rate\": tune.grid_search([1e-5]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myunchiz\u001b[0m (\u001b[33myunchiz-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/wandb/run-20250415_104734-qcpiyjpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl' target=\"_blank\">scarlet-smoke-24</a></strong> to <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f40ee2353a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Mlops-summary\", entity=\"yunchiz-new-york-university\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-15 10:50:34</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:45.58        </td></tr>\n",
       "<tr><td>Memory:      </td><td>34.1/629.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/10 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  eval_loss</th><th style=\"text-align: right;\">  eval_rouge1</th><th style=\"text-align: right;\">  eval_rouge2</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_04641_00000</td><td>TERMINATED</td><td>172.17.0.2:15050</td><td style=\"text-align: right;\">          1e-05</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         141.888</td><td style=\"text-align: right;\">    1.32029</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">            0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Currently logged in as: yunchiz (yunchiz-new-york-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Tracking run with wandb version 0.19.9\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-04-15_10-47-35_553542_13907/artifacts/2025-04-15_10-47-48/ray_test_epoch_2/working_dirs/train_fn_04641_00000_0_learning_rate=0.0000_2025-04-15_10-47-48/wandb/run-20250415_104805-q97xhpey\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: Syncing run ray_test_epoch_2\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/yunchiz-new-york-university/huggingface\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/yunchiz-new-york-university/huggingface/runs/q97xhpey\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]\n",
      "  2%|â–         | 1/62 [00:01<01:49,  1.79s/it]\n",
      "  3%|â–         | 2/62 [00:02<01:09,  1.16s/it]\n",
      "  5%|â–         | 3/62 [00:03<01:00,  1.03s/it]\n",
      "  6%|â–‹         | 4/62 [00:04<00:50,  1.15it/s]\n",
      "  8%|â–Š         | 5/62 [00:04<00:44,  1.28it/s]\n",
      " 10%|â–‰         | 6/62 [00:05<00:40,  1.38it/s]\n",
      " 11%|â–ˆâ–        | 7/62 [00:05<00:37,  1.45it/s]\n",
      " 13%|â–ˆâ–        | 8/62 [00:06<00:35,  1.51it/s]\n",
      " 15%|â–ˆâ–        | 9/62 [00:07<00:34,  1.54it/s]\n",
      " 16%|â–ˆâ–Œ        | 10/62 [00:07<00:33,  1.57it/s]\n",
      " 18%|â–ˆâ–Š        | 11/62 [00:08<00:32,  1.57it/s]\n",
      " 19%|â–ˆâ–‰        | 12/62 [00:08<00:31,  1.58it/s]\n",
      " 21%|â–ˆâ–ˆ        | 13/62 [00:09<00:30,  1.59it/s]\n",
      " 23%|â–ˆâ–ˆâ–       | 14/62 [00:10<00:30,  1.59it/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 15/62 [00:10<00:29,  1.60it/s]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 16/62 [00:11<00:28,  1.60it/s]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 17/62 [00:12<00:27,  1.61it/s]\n",
      " 29%|â–ˆâ–ˆâ–‰       | 18/62 [00:12<00:27,  1.61it/s]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 19/62 [00:13<00:26,  1.61it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 20/62 [00:13<00:26,  1.61it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 21/62 [00:14<00:25,  1.61it/s]\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 22/62 [00:15<00:24,  1.61it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/62 [00:15<00:24,  1.61it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 24/62 [00:16<00:23,  1.61it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 25/62 [00:17<00:22,  1.61it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/62 [00:17<00:22,  1.61it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/62 [00:18<00:21,  1.61it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 28/62 [00:18<00:21,  1.61it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 29/62 [00:19<00:20,  1.61it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/62 [00:20<00:19,  1.60it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 31/62 [00:20<00:19,  1.59it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/62 [00:20<00:14,  2.05it/s]\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 12%|â–ˆâ–        | 3/25 [00:00<00:00, 22.88it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 24%|â–ˆâ–ˆâ–       | 6/25 [00:00<00:01, 17.60it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:00<00:01,  9.72it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:00<00:01, 10.96it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:00<00:01, 11.92it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:01<00:00, 12.65it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:01<00:00, 13.11it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:01<00:00,  8.89it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:01<00:00,  7.92it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:02<00:00,  8.62it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:02<00:00,  5.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™ï¼š'tuple' object has no attribute 'shape'\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m {'eval_loss': 1.9222586154937744, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_runtime': 11.9726, 'eval_samples_per_second': 16.705, 'eval_steps_per_second': 2.088, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/62 [00:32<00:14,  2.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:11<00:00,  5.76it/s]\u001b[A\n",
      "                                               \u001b[A/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m   warnings.warn(\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/62 [00:40<02:59,  6.18s/it]\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/62 [00:41<02:06,  4.51s/it]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 35/62 [00:41<01:30,  3.35s/it]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 36/62 [00:42<01:06,  2.57s/it]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 37/62 [00:43<00:49,  1.99s/it]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/62 [00:43<00:38,  1.59s/it]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/62 [00:44<00:29,  1.30s/it]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/62 [00:44<00:24,  1.11s/it]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/62 [00:45<00:20,  1.03it/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 42/62 [00:46<00:17,  1.14it/s]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 43/62 [00:46<00:15,  1.24it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 44/62 [00:47<00:13,  1.32it/s]\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/62 [00:48<00:12,  1.37it/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/62 [00:48<00:11,  1.41it/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 47/62 [00:49<00:10,  1.45it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 48/62 [00:50<00:09,  1.48it/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 49/62 [00:50<00:08,  1.51it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 50/62 [00:51<00:07,  1.53it/s]\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 51/62 [00:52<00:07,  1.54it/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/62 [00:52<00:06,  1.55it/s]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 53/62 [00:53<00:05,  1.56it/s]\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 54/62 [00:54<00:05,  1.56it/s]\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 55/62 [00:54<00:04,  1.56it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 56/62 [00:55<00:03,  1.56it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 57/62 [00:55<00:03,  1.56it/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/62 [00:56<00:02,  1.55it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 59/62 [00:57<00:01,  1.55it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 60/62 [00:57<00:01,  1.55it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 61/62 [00:58<00:00,  1.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:59<00:00,  1.55it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 12%|â–ˆâ–        | 3/25 [00:00<00:00, 22.54it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 24%|â–ˆâ–ˆâ–       | 6/25 [00:00<00:01, 17.59it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:00<00:01, 16.41it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:00<00:00, 15.24it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:00<00:00, 14.97it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:01<00:00, 14.65it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:01<00:00, 14.39it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:01<00:00, 14.21it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:01<00:00, 14.09it/s]\u001b[A\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m \n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:01<00:00, 14.00it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™ï¼š'tuple' object has no attribute 'shape'\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m {'eval_loss': 1.3202884197235107, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_runtime': 10.3793, 'eval_samples_per_second': 19.269, 'eval_steps_per_second': 2.409, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:09<00:00,  1.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:10<00:00, 14.00it/s]\u001b[A\n",
      "                                               \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m {'train_runtime': 82.2217, 'train_samples_per_second': 24.324, 'train_steps_per_second': 0.754, 'train_loss': 3.1087245325888357, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [01:20<00:00,  1.30s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      " 12%|â–ˆâ–        | 3/25 [00:00<00:01, 17.42it/s]\n",
      " 20%|â–ˆâ–ˆ        | 5/25 [00:00<00:01, 15.56it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:00<00:01, 15.34it/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:00<00:01, 15.04it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:00<00:00, 14.78it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:00<00:00, 14.69it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:01<00:00, 14.62it/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:01<00:00, 14.57it/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:01<00:00, 14.46it/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:01<00:00, 14.32it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:01<00:00, 14.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 14.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=15050)\u001b[0m è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™ï¼š'tuple' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:09<00:00,  2.57it/s]\n",
      "2025-04-15 10:50:10,267\tWARNING experiment_state.py:233 -- Saving the experiment state (which holds a global view of trial statuses and is used to restore the experiment) has already taken 30.24 seconds, which may cause consistency issues upon restoration if your driver script ungracefully exits.\n",
      "This could be due to a large number of trials, large logfiles from lots of reported metrics, or throttling from the remote storage if uploading too frequently.\n",
      "You may want to consider switching the `RunConfig(storage_filesystem)` to a more performant storage backend such as s3fs for a S3 storage path.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_SLOW_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a higher value than the current threshold (30.0).\n",
      "\u001b[36m(train_fn pid=15050)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/autodl-tmp/ray_results/ray_test_epoch_2/train_fn_04641_00000_0_learning_rate=0.0000_2025-04-15_10-47-48/checkpoint_000000)\n",
      "2025-04-15 10:50:34,341\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/autodl-tmp/ray_results/ray_test_epoch_2' in 16.0637s.\n",
      "2025-04-15 10:50:34,351\tINFO tune.py:1041 -- Total run time: 165.76 seconds (149.51 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "storage_path = f\"file://{current_dir}/ray_results\"\n",
    "\n",
    "train_fn_with_params = tune.with_parameters(train_fn, model=model, train_dataset=train_subset, eval_dataset=eval_subset)\n",
    "ray.init(ignore_reinit_error=True)  # Initialize Ray\n",
    "analysis = tune.run(\n",
    "    train_fn_with_params,  # The training function that Ray Tune will use\n",
    "    config=search_space,  # The search space of hyperparameters\n",
    "    # resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
    "    resources_per_trial={\"cpu\": 0, \"gpu\": 1},\n",
    "    num_samples=1,  # Number of trials (hyperparameter combinations)\n",
    "    verbose=1,  # Verbosity level of Ray Tune\n",
    "    storage_path=storage_path,\n",
    "    name=\"ray_test_epoch_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ä½³æ¨¡å‹è·¯å¾„ï¼š/tmp/checkpoint_tmp_f69b548edfae4ccb947b243d69a34093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_size = 200  # Same for evaluation dataset\n",
    "test_dataset = tokenized_datasets[\"test\"].select(range(test_size))\n",
    "\n",
    "best_trial = analysis.get_best_trial(metric=\"eval_accuracy\", mode=\"max\")\n",
    "\n",
    "# è·å–æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆé€šè¿‡ checkpoint å±æ€§ï¼‰\n",
    "best_checkpoint = best_trial.checkpoint\n",
    "best_checkpoint_dir = best_checkpoint.to_directory()  # æå–æ£€æŸ¥ç‚¹ç›®å½•\n",
    "print(f\"æœ€ä½³æ¨¡å‹è·¯å¾„ï¼š{best_checkpoint_dir}\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "from transformers import AutoModel\n",
    "best_model = BartForConditionalGeneration.from_pretrained(best_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=best_model,\n",
    "#     args=TrainingArguments(output_dir=\"./tmp\"),  # ä¸´æ—¶ç›®å½•ï¼Œä»…ç”¨äºé¢„æµ‹\n",
    "# )\n",
    "\n",
    "# predictions = trainer.predict(test_dataset)\n",
    "# predictions_logits = predictions.predictions\n",
    "# # predicted_labels = np.argmax(predictions_logits, axis=1)\n",
    "\n",
    "# accuracy = accuracy_score(test_dataset[\"labels\"], predictions_logits)\n",
    "# f1 = f1_score(test_dataset[\"labels\"], predictions_logits, average=\"macro\")\n",
    "\n",
    "# results = rouge.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# print(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": 128,          # ç”Ÿæˆæ‘˜è¦çš„æœ€å¤§é•¿åº¦\n",
    "    \"min_length\": 30,           # ç”Ÿæˆæ‘˜è¦çš„æœ€å°é•¿åº¦\n",
    "    \"num_beams\": 4,             # Beam Search çš„ beam æ•°\n",
    "    \"length_penalty\": 2.0,      # é•¿åº¦æƒ©ç½šç³»æ•°ï¼ˆ>1é¼“åŠ±æ›´é•¿ï¼Œ<1é¼“åŠ±æ›´çŸ­ï¼‰\n",
    "    \"no_repeat_ngram_size\": 3,  # ç¦æ­¢é‡å¤çš„ n-gram å¤§å°\n",
    "    \"early_stopping\": True,     # æ˜¯å¦æå‰åœæ­¢ç”Ÿæˆ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testset(test_data, model, tokenizer):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æµ‹è¯•é›†å¹¶è¿”å› ROUGE æŒ‡æ ‡\n",
    "    Args:\n",
    "        test_data (Dataset): é¢„å¤„ç†åçš„æµ‹è¯•æ•°æ®é›†ï¼ˆéœ€åŒ…å« \"article\" å’Œ \"highlights\"ï¼‰\n",
    "        model (PreTrainedModel): åŠ è½½çš„æœ€ä½³æ¨¡å‹\n",
    "        tokenizer (PreTrainedTokenizer): å¯¹åº”çš„ tokenizer\n",
    "    Returns:\n",
    "        dict: ROUGE æŒ‡æ ‡ç»“æœ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # å­˜å‚¨é¢„æµ‹å’ŒçœŸå®æ‘˜è¦\n",
    "        pred_summaries = []\n",
    "        true_summaries = []\n",
    "        \n",
    "        # æ‰¹é‡ç”Ÿæˆé¢„æµ‹\n",
    "        batch_size = 8  # æ ¹æ® GPU æ˜¾å­˜è°ƒæ•´\n",
    "        for i in range(0, len(test_data), batch_size):\n",
    "            batch = test_data.select(range(i, min(i+batch_size, len(test_data))))\n",
    "            \n",
    "            # ç¼–ç è¾“å…¥æ–‡æœ¬\n",
    "            inputs = tokenizer(\n",
    "                batch[\"article\"],\n",
    "                max_length=1024,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # ç”Ÿæˆæ‘˜è¦\n",
    "            with torch.no_grad():\n",
    "                summaries = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    **gen_kwargs\n",
    "                )\n",
    "            \n",
    "            # è§£ç é¢„æµ‹å’ŒçœŸå®æ‘˜è¦\n",
    "            decoded_preds = tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
    "            decoded_labels = [highlight for highlight in batch[\"highlights\"]]\n",
    "            \n",
    "            pred_summaries.extend(decoded_preds)\n",
    "            true_summaries.extend(decoded_labels)\n",
    "            \n",
    "            print(f\"Processed {i + batch_size}/{len(test_data)} samples\")\n",
    "            \n",
    "        # è®¡ç®— ROUGE æŒ‡æ ‡\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        results = rouge.compute(\n",
    "            predictions=pred_summaries,\n",
    "            references=true_summaries,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # æ ¼å¼åŒ–ç»“æœï¼ˆä¿ç•™4ä½å°æ•°ï¼‰\n",
    "        return {\n",
    "            \"rouge1\": round(results[\"rouge1\"], 4),\n",
    "            \"rouge2\": round(results[\"rouge2\"], 4),\n",
    "            \"rougeL\": round(results[\"rougeL\"], 4),\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8/200 samples\n",
      "Processed 16/200 samples\n",
      "Processed 24/200 samples\n",
      "Processed 32/200 samples\n",
      "Processed 40/200 samples\n",
      "Processed 48/200 samples\n",
      "Processed 56/200 samples\n",
      "Processed 64/200 samples\n",
      "Processed 72/200 samples\n",
      "Processed 80/200 samples\n",
      "Processed 88/200 samples\n",
      "Processed 96/200 samples\n",
      "Processed 104/200 samples\n",
      "Processed 112/200 samples\n",
      "Processed 120/200 samples\n",
      "Processed 128/200 samples\n",
      "Processed 136/200 samples\n",
      "Processed 144/200 samples\n",
      "Processed 152/200 samples\n",
      "Processed 160/200 samples\n",
      "Processed 168/200 samples\n",
      "Processed 176/200 samples\n",
      "Processed 184/200 samples\n",
      "Processed 192/200 samples\n",
      "Processed 200/200 samples\n",
      "\n",
      "æµ‹è¯•é›†è¯„ä¼°ç»“æœ:\n",
      "ROUGE-1: 0.3567\n",
      "ROUGE-2: 0.1528\n",
      "ROUGE-L: 0.263\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆå‡è®¾å·²é¢„å¤„ç†ï¼‰\n",
    "    # test_size = 200\n",
    "    # test_dataset = tokenized_datasets[\"test\"].select(range(test_size))\n",
    "    \n",
    "    # # åŠ è½½æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "    # best_trial = analysis.get_best_trial(metric=\"metrics\", mode=\"max\")  # è¯·æ ¹æ®å®é™…æŒ‡æ ‡åç§°ä¿®æ”¹\n",
    "    # best_checkpoint_dir = best_trial.checkpoint.to_directory()\n",
    "    \n",
    "    # # åŠ è½½æ¨¡å‹å’Œ tokenizer\n",
    "    # tokenizer = BartTokenizer.from_pretrained(best_checkpoint_dir)\n",
    "    # model = BartForConditionalGeneration.from_pretrained(best_checkpoint_dir)\n",
    "    \n",
    "    # æ‰§è¡Œè¯„ä¼°\n",
    "    results = evaluate_testset(test_dataset, best_model, tokenizer)\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\næµ‹è¯•é›†è¯„ä¼°ç»“æœ:\")\n",
    "    print(f\"ROUGE-1: {results['rouge1']}\")\n",
    "    print(f\"ROUGE-2: {results['rouge2']}\")\n",
    "    print(f\"ROUGE-L: {results['rougeL']}\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"æ•°æ®åŠ è½½é”™è¯¯: è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦åŒ…å« 'article' å’Œ 'highlights' å­—æ®µ - {str(e)}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"æ¨¡å‹åŠ è½½é”™è¯¯: æ£€æŸ¥ç‚¹è·¯å¾„ {best_checkpoint_dir} ä¸å­˜åœ¨ - {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"æœªçŸ¥é”™è¯¯: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-smoke-24</strong> at: <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary/runs/qcpiyjpl</a><br> View project at: <a href='https://wandb.ai/yunchiz-new-york-university/Mlops-summary' target=\"_blank\">https://wandb.ai/yunchiz-new-york-university/Mlops-summary</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_104734-qcpiyjpl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def summarize(text):\n",
    "#     inputs = tokenizer([text], max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "#     summary_ids = best_model.generate(inputs[\"input_ids\"], max_length=128, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# print(summarize(\"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "# A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "# Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "# In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "# Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "# 2010 marriage license application, according to court documents.\n",
    "# Prosecutors said the marriages were part of an immigration scam.\n",
    "# On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "# After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "# Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "# All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "# Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "# Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "# The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "# Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "# Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "# If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. If convicted, she faces up to four years in prison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
